# ğŸ§  Local RAG Engine (NestJS)

A **deterministic, production-style Retrieval-Augmented Generation (RAG) backend** built with **NestJS**, **local embeddings**, **external vector storage**, **agentic validation**, and **streaming answers**.

---

## âœ¨ Key Capabilities

- âœ… Deterministic document ingestion (CLI-based)
- ğŸŒ Language-aware document handling (folder-based)
- ğŸ§  Local embeddings using **Xenova**
- ğŸ—„ External vector storage using **ChromaDB**
- âš¡ Redis-based deduplication and chat state
- ğŸ¤– Agentic query validation (LLM + evidence)
- ğŸ’¬ Multi-turn chat with clarification handling
- ğŸŒŠ Streaming answers using **Gemini**
- ğŸ§± Clean, modular NestJS architecture

---

## ğŸ§± High-Level Architecture

PDF Files
â†“
PDF Parser
â†“
Text Splitter (sliding window)
â†“
Local Embeddings (Xenova)
â†“
Vector Store (ChromaDB)

### Chat flow:

User Query
â†“
Vector Signal Check
â†“
Agentic Validation (with snippets)
â†“
Retrieval (top-K chunks)
â†“
Context Assembly
â†“
LLM Streaming Answer (Gemini)

### ğŸ“ Project Structure

rag-engine/
â”œâ”€â”€ data/
â”‚ â””â”€â”€ documents/
â”‚ â”œâ”€â”€ en/
â”‚ â”œâ”€â”€ ml/
â”‚ â””â”€â”€ de/
â”‚
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ ai/ # Core AI infrastructure
â”‚ â”œâ”€â”€ chat-llm/ # LLM adapters & prompts
â”‚ â”œâ”€â”€ chat-agents/ # Agentic reasoning (validation)
â”‚ â”œâ”€â”€ chat-retrieval/ # Vector retrieval + context assembly
â”‚ â”œâ”€â”€ chat/ # Chat orchestration & state
â”‚ â”œâ”€â”€ cli/ # CLI ingestion
â”‚ â”œâ”€â”€ redis/ # Redis integration
â”‚ â””â”€â”€ app.module.ts
â”‚
â”œâ”€â”€ docker-compose.yml # ChromaDB + Redis
â”œâ”€â”€ package.json
â”œâ”€â”€ tsconfig.json
â”œâ”€â”€ .env

## ğŸŒ Language Handling (Deterministic)

Documents are grouped by **folder name**, which is the **source of truth** for language.
  - data/documents/en/.pdf â†’ English
  - data/documents/ml/.pdf â†’ Malayalam
  - data/documents/de/*.pdf â†’ German

Why this approach?

- âœ” No language detection cost
- âœ” No misclassification
- âœ” Deterministic behavior
- âœ” Easy to scale

---

## ğŸ§© Core Components (What Each Does)

### 1ï¸âƒ£ EmbeddingsService
- Uses **Xenova/all-MiniLM-L6-v2**
- Runs locally (no cloud dependency)
- Model is loaded once and reused

---

### 2ï¸âƒ£ ChromaService
- Thin client to external **ChromaDB**
- Handles:
  - Upserts
  - Similarity search
- Acts as an adapter (normalizes external data)

---

### 3ï¸âƒ£ VectorSignalService
- Cheap probe to check:
  > â€œDoes the corpus contain anything related to this query?â€
- Prevents unnecessary LLM calls

---

### 4ï¸âƒ£ QueryValidatorAgent
- LLM-based agent
- Validates queries using **retrieved snippets**
- Handles:
  - Ambiguity
  - Out-of-scope queries
  - Clarification requests

---

### 5ï¸âƒ£ RetrieverService
- Performs real similarity search (top-K)
- No thresholds, no guessing
- Pure retrieval logic

---

### 6ï¸âƒ£ ContextAssemblerService
- Builds a safe, bounded context
- Prevents token overflow
- Preserves document sources

---

### 7ï¸âƒ£ AnswerGeneratorService
- Streams answers using **Gemini**
- No hallucination:
  - Answers ONLY from provided context
- Retry + backoff on transient failures

---

### 8ï¸âƒ£ ChatService (Orchestrator)
Coordinates the full flow:
Signal Check
â†’ Validation
â†’ Clarification Handling
â†’ Retrieval
â†’ Context Assembly
â†’ Streaming Answer

---

## ğŸ” Ingestion Workflow (CLI)

### Command

```bash
npx ts-node src/cli.ts ingest:documents
```
#### What happens
  1. Crawl language folders
  2. Parse PDFs
  3. Split text (sliding window)
  4. Hash chunks (SHA-256)
  5. Deduplicate using Redis 
  6. Embed new chunks (Xenova)
  7. Upsert to ChromaDB

## ğŸ’¬ Chat Workflow (Multi-Turn)

#### Example

User: What is life?

System: Did you mean LIFE Mission (Livelihood Inclusion and Financial Empowerment Mission)?

(State stored in Redis)

User: yes

System:
  - Resolves clarification
  - Rewrites query internally
  - Retrieves context
  - Streams grounded answer

âœ” No hallucination
âœ” Natural chat experience
âœ” Deterministic state handling

## ğŸ§  Design Principles

  - Determinism over guessing
  - Evidence before reasoning
  - Clarify before answering
  - State in backend, not in LLM
  - Adapters isolate external systems
  - Fail closed, never hallucinate


## ğŸš€ Getting Started
Prerequisites:
  - Docker
  - Node.js 18+
  - Docker & Docker Compose
  - Redis
  - ChromaDB
  
#### Environment Variables
```bash
GEMINI_API_KEY=your_api_key
CHROMA_HOST=http://localhost:8000
```

#### Start infrastructure
```bash
docker compose up -d
```

#### Run ingestion
```bash
npx ts-node src/cli.ts ingest:documents
```

#### Start API (dev)
```bash
npm run start:dev
```